{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTffeNKw8SDbu8sbmCzNKg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattadeb34/FACT_Data_Extraction_Scientific_Papers/blob/main/LLm_embeddings_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPDKioWiT9Qa"
      },
      "outputs": [],
      "source": [
        "!pip install pandas sentence-transformers scikit-learn matplotlib seaborn umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "bYl75rRWUtyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and Configuration (Modified)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai  # NEW: Import the Google AI library\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import matplotlib.pyplot as plt\n",
        "#from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# --- Configuration ---\n",
        "CSV_PATH = \"master_conductivity_data.csv\"\n",
        "OUTPUT_FILENAME = \"conductivity_pca_analysis.png\"\n",
        "\n",
        "# NEW: Choose your embedding provider here (\"openai\" or \"gemini\")\n",
        "EMBEDDING_PROVIDER = \"gemini\"\n",
        "\n",
        "# Select the model based on the provider\n",
        "if EMBEDDING_PROVIDER == \"openai\":\n",
        "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "elif EMBEDDING_PROVIDER == \"gemini\":\n",
        "    EMBEDDING_MODEL = \"gemini-embedding-001\"\n",
        "# Configuration dictionary for models and their known dimensions\n",
        "MODEL_CONFIG = {\n",
        "    \"text-embedding-ada-002\": 1536,\n",
        "    \"gemini-embedding-001\": 1536  # We will request this size for consistency\n",
        "}"
      ],
      "metadata": {
        "id": "2lkfVU5USUVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: API Key and Client Initialization (Modified)\n",
        "\n",
        "api_key = None\n",
        "client = None\n",
        "\n",
        "if EMBEDDING_PROVIDER == \"openai\":\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "        print(\"Retrieved OpenAI API key from Colab secrets.\")\n",
        "    except (ImportError, KeyError):\n",
        "        print(\"Not in Colab, checking environment variable for OPENAI_API_KEY...\")\n",
        "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OpenAI API key not found.\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    print(\"OpenAI client initialized.\")\n",
        "\n",
        "elif EMBEDDING_PROVIDER == \"gemini\":\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('GOOGLE_API_KEY')\n",
        "        print(\"Retrieved Google Gemini API key from Colab secrets.\")\n",
        "    except (ImportError, KeyError):\n",
        "        print(\"Not in Colab, checking environment variable for GOOGLE_GEMINI_API_KEY...\")\n",
        "        api_key = os.environ.get(\"GOOGLE_GEMINI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Google Gemini API key not found.\")\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\"Google Gemini client configured.\")"
      ],
      "metadata": {
        "id": "SotL_cS_XE8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Helper Functions (Updated)\n",
        "\n",
        "def _parse_temperature_value(value):\n",
        "    \"\"\"\n",
        "    Parses a single temperature string into a numerical Celsius value.\n",
        "    \"\"\"\n",
        "    if pd.isna(value): return np.nan\n",
        "    value_str = str(value).lower()\n",
        "    if 'rt' in value_str or 'room' in value_str: return 25.0\n",
        "    if 'assumed' in value_str or 'not specified' in value_str: return np.nan\n",
        "    numbers = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', value_str)\n",
        "    if numbers:\n",
        "        temp = float(numbers[0])\n",
        "        if temp > 200: return temp - 273.15\n",
        "        return temp\n",
        "    return np.nan\n",
        "\n",
        "def clean_and_standardize_temperature(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies the temperature parsing logic to create a standardized 'temperature_celsius' column.\n",
        "    \"\"\"\n",
        "    print(\"Standardizing temperature column...\")\n",
        "    df['temperature_celsius'] = df['temperature'].apply(_parse_temperature_value)\n",
        "    parsed_count = df['temperature_celsius'].notna().sum()\n",
        "    print(f\"  -> Successfully parsed {parsed_count} / {len(df)} temperature entries.\")\n",
        "    return df\n",
        "\n",
        "# In Cell 3 (Helper Functions)\n",
        "\n",
        "def analyze_pca_by_word_correlation(df: pd.DataFrame, pca: PCA, provider: str, model_name: str, num_components: int = 2):\n",
        "    \"\"\"\n",
        "    Analyzes PCA components by correlating them directly with the embeddings of\n",
        "    individual words found within the dataset.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Interpreting PCA via Word-Component Correlation ---\")\n",
        "\n",
        "    stop_words = set(ENGLISH_STOP_WORDS)\n",
        "    custom_stop_words = {'material', 'description', 'processing', 'method', 'na'}\n",
        "    stop_words.update(custom_stop_words)\n",
        "\n",
        "    interpretation_df = df[~df['material_description'].str.contains(\"Cited Work\", na=False)]\n",
        "    if interpretation_df.empty:\n",
        "        print(\"Warning: No primary data found for interpretation.\")\n",
        "        return\n",
        "\n",
        "    all_text = ' '.join(interpretation_df['feature_text'])\n",
        "    words = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', all_text.lower()))\n",
        "    vocabulary = sorted([word for word in words if word not in stop_words])\n",
        "    print(f\"Built a vocabulary of {len(vocabulary)} unique terms from the data.\")\n",
        "\n",
        "    # MODIFIED: Use the generic dispatcher function instead of the OpenAI-specific one\n",
        "    vocab_embeddings = get_embeddings(provider, model_name, vocabulary)\n",
        "\n",
        "    for i in range(num_components):\n",
        "        component_vector = pca.components_[i]\n",
        "        alignment_scores = np.dot(vocab_embeddings, component_vector)\n",
        "        sorted_indices = np.argsort(alignment_scores)\n",
        "\n",
        "        positive_pole_words = [vocabulary[j] for j in sorted_indices[-5:][::-1]]\n",
        "        negative_pole_words = [vocabulary[j] for j in sorted_indices[:5]]\n",
        "\n",
        "        print(f\"\\n[+] Component {i+1} appears to distinguish between:\")\n",
        "        print(f\"    - High values (Positive Pole) are defined by: {positive_pole_words}\")\n",
        "        print(f\"    - Low values (Negative Pole) are defined by: {negative_pole_words}\")"
      ],
      "metadata": {
        "id": "kQuftk84W1CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Helper Functions (Modified)\n",
        "\n",
        "# --- (Your _parse_temperature_value and clean_and_standardize_temperature functions remain here) ---\n",
        "\n",
        "def get_openai_embeddings(texts: list[str], client: OpenAI, model_name: str, batch_size: int = 500) -> np.ndarray:\n",
        "    \"\"\"Generates embeddings using OpenAI's API.\"\"\"\n",
        "    embedding_dim = MODEL_CONFIG.get(model_name, 1536) # Default to 1536 if unknown\n",
        "    all_embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Generating {model_name} Embeddings\"):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        try:\n",
        "            response = client.embeddings.create(input=batch, model=model_name)\n",
        "            embeddings = [item.embedding for item in response.data]\n",
        "            all_embeddings.extend(embeddings)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            all_embeddings.extend([[0] * embedding_dim] * len(batch))\n",
        "    return np.array(all_embeddings)\n",
        "\n",
        "def get_gemini_embeddings(texts: list[str], model_name: str, batch_size: int = 100) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates embeddings using Gemini's API, following recommended practices.\n",
        "    \"\"\"\n",
        "    embedding_dim = MODEL_CONFIG.get(model_name, 1536)\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Generating {model_name} Embeddings\"):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        try:\n",
        "            # Recommended practice: Specify task_type and output_dimensionality [cite: 1149, 1207]\n",
        "            result = genai.embed_content(\n",
        "                model=model_name,\n",
        "                content=batch,\n",
        "                task_type=\"SEMANTIC_SIMILARITY\",\n",
        "                output_dimensionality=embedding_dim\n",
        "            )\n",
        "\n",
        "            # Recommended practice: Normalize embeddings for dimensions other than 3072 [cite: 1229]\n",
        "            for embedding in result['embedding']:\n",
        "                embedding_np = np.array(embedding)\n",
        "                normed_embedding = embedding_np / np.linalg.norm(embedding_np)\n",
        "                all_embeddings.append(normed_embedding)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            all_embeddings.extend([[0] * embedding_dim] * len(batch))\n",
        "\n",
        "    return np.array(all_embeddings)\n",
        "\n",
        "def get_embeddings(provider: str, model_name: str, texts: list[str]) -> np.ndarray:\n",
        "    \"\"\"Dispatcher function to call the correct embedding service.\"\"\"\n",
        "    if provider == \"openai\":\n",
        "        return get_openai_embeddings(texts, client, model_name)\n",
        "    elif provider == \"gemini\":\n",
        "        return get_gemini_embeddings(texts, model_name)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {provider}. Choose 'openai' or 'gemini'.\")\n"
      ],
      "metadata": {
        "id": "5RMwBsMNS1qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the master CSV\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(f\"Loaded {len(df)} total data points from CSV.\")"
      ],
      "metadata": {
        "id": "NDJjI5uDNZEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "NXyrMJB1Nc7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_paper_titles=df['paper_title'].unique()\n",
        "for title in unique_paper_titles:\n",
        "    print(title)"
      ],
      "metadata": {
        "id": "B5ae2CVmcIDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['paper_title']"
      ],
      "metadata": {
        "id": "G-MazOWu80B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NEW STEP 1: Clean DOIs and Deduplicate Papers ---\n",
        "# This step ensures each paper is only represented once in our analysis dataset.\n",
        "# MODIFIED: Use 'paper_title' for deduplication instead of 'paper_doi'\n",
        "df.dropna(subset=['paper_title'], inplace=True)\n",
        "unique_papers_before = df['paper_title'].nunique()\n",
        "# We keep the first set of entries for each unique paper based on 'paper_title'\n",
        "df_unique_papers = df.drop_duplicates(subset=['paper_title'], keep='first').copy()\n",
        "print(f\"Filtered down to {len(df_unique_papers)} entries representing {unique_papers_before} unique papers based on paper title.\")\n",
        "# ---"
      ],
      "metadata": {
        "id": "P-wXrBHwNq9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unique_papers"
      ],
      "metadata": {
        "id": "ctcUwIcd9LKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NEW STEP 2: Clean and FILTER for Room Temperature ---\n",
        "# We will now work with the deduplicated DataFrame\n",
        "df_cleaned = clean_and_standardize_temperature(df_unique_papers)\n"
      ],
      "metadata": {
        "id": "xPzivLiAOqnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame containing ONLY room temperature data (20-30°C)\n",
        "rt_df = df_cleaned[(df_cleaned['temperature_celsius'] >= 20) & (df_cleaned['temperature_celsius'] <= 30)].copy()\n",
        "print(f\"Further filtered to {len(rt_df)} data points measured at Room Temperature (20-30°C).\")"
      ],
      "metadata": {
        "id": "-AJOL5RjOu7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rt_df['temperature_celsius'].unique())"
      ],
      "metadata": {
        "id": "2R9sJHXaOxt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not rt_df.empty:\n",
        "    # Create the feature text for embedding using only the RT data\n",
        "    rt_df['feature_text'] = (\n",
        "        \"Material: \" + rt_df['full_name'].fillna('') +\n",
        "        \". Description: \" + rt_df['material_description'].fillna('') +\n",
        "        \". Processing: \" + rt_df['processing_method'].fillna('')\n",
        "    )\n",
        "\n",
        "    # --- THIS IS THE ONLY CALL YOU NEED ---\n",
        "    # Generate embeddings using the dispatcher function\n",
        "    embeddings = get_embeddings(\n",
        "        provider=EMBEDDING_PROVIDER,\n",
        "        model_name=EMBEDDING_MODEL,\n",
        "        texts=rt_df['feature_text'].tolist()\n",
        "    )\n",
        "    # ------------------------------------\n",
        "\n",
        "    # Filter out any rows where embedding might have failed\n",
        "    valid_mask = np.all(embeddings != 0, axis=1)\n",
        "    rt_df = rt_df[valid_mask].copy()\n",
        "    embeddings = embeddings[valid_mask]\n",
        "\n",
        "    print(f\"\\nSuccessfully generated embeddings for {len(rt_df)} room temperature data points.\")\n",
        "else:\n",
        "    print(\"\\nNo room temperature data found after filtering. Cannot proceed with embedding.\")\n",
        "\n",
        "# The 'rt_df' DataFrame is now ready for the next cell (PCA and visualization)"
      ],
      "metadata": {
        "id": "ntpzpVr0ZGQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Perform PCA and Interpret Results (Corrected)\n",
        "\n",
        "# Perform PCA on the embeddings of the filtered room-temperature data\n",
        "print(\"\\nReducing embedding dimensions with PCA...\")\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Add PCA columns to the 'rt_df' DataFrame\n",
        "rt_df['pca_1'] = embeddings_2d[:, 0]\n",
        "rt_df['pca_2'] = embeddings_2d[:, 1]\n",
        "\n",
        "# --- NEW: Programmatic Log Calculation ---\n",
        "# First, try to calculate the log directly\n",
        "# We temporarily suppress the \"divide by zero\" warning that numpy will raise for log(0)\n",
        "with np.errstate(divide='ignore'):\n",
        "    log_values = np.log10(rt_df['ionic_conductivity_S_per_cm'].values)\n",
        "\n",
        "# Check if the operation produced any -inf values (the result of log(0))\n",
        "if np.isneginf(log_values).any():\n",
        "    print(\"Warning: Zero or non-positive conductivity values found. Adding a small constant for log scaling.\")\n",
        "    # If -inf exists, recalculate by adding a small constant to ensure numerical stability\n",
        "    rt_df['log_conductivity'] = np.log10(rt_df['ionic_conductivity_S_per_cm'] + 1e-15)\n",
        "else:\n",
        "    # If no errors occurred, use the directly calculated values\n",
        "    rt_df['log_conductivity'] = log_values\n",
        "# --- END NEW LOGIC ---\n",
        "\n",
        "# Run the updated analysis function, passing in the correct DataFrame and arguments\n",
        "analyze_pca_by_word_correlation(rt_df, pca, EMBEDDING_PROVIDER, EMBEDDING_MODEL)"
      ],
      "metadata": {
        "id": "CjACizWtZK9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rt_df.head(5))"
      ],
      "metadata": {
        "id": "7Kz-Yk_1As3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Visualize and Save the Plot (Corrected)\n",
        "\n",
        "# Create and save the final visualization using the 'rt_df' DataFrame\n",
        "print(f\"\\nCreating visualization and saving to '{OUTPUT_FILENAME}'...\")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "scatter = ax.scatter(\n",
        "    x=rt_df['pca_1'], y=rt_df['pca_2'], c=rt_df['log_conductivity'],\n",
        "    cmap='viridis', s=50, alpha=0.8\n",
        ")\n",
        "\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Log10(Ionic Conductivity [S/cm]) @ Room Temp', fontsize=12) # Updated label for clarity\n",
        "\n",
        "ax.set_title('PCA of Room Temperature Materials (ada-002) vs. Ionic Conductivity', fontsize=16, pad=20) # Updated title\n",
        "ax.set_xlabel('Principal Component 1', fontsize=12)\n",
        "ax.set_ylabel('Principal Component 2', fontsize=12)\n",
        "\n",
        "plt.savefig(OUTPUT_FILENAME, dpi=300, bbox_inches='tight')\n",
        "print(\"Done.\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BuvElGDgZOC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly"
      ],
      "metadata": {
        "id": "P57SNFomb6lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create an Interactive Visualization with Wrapped Text (Corrected)\n",
        "import plotly.express as px\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_for_plotly(text: str, width: int = 80) -> str:\n",
        "    \"\"\"Wraps a long string into multiple lines using HTML <br> tags.\"\"\"\n",
        "    # textwrap.wrap splits the string into a list of lines\n",
        "    # '<br>'.join(...) then joins them back together with the HTML line break tag\n",
        "    return '<br>'.join(textwrap.wrap(text, width=width))\n",
        "\n",
        "# --- Prepare Data for Hovering ---\n",
        "# Apply the wrapping function to the feature_text column to create a new column\n",
        "# with HTML-formatted text for our tooltip.\n",
        "rt_df['feature_text_wrapped'] = rt_df['feature_text'].apply(wrap_text_for_plotly)\n",
        "\n",
        "# Create the interactive scatter plot\n",
        "print(f\"\\nCreating interactive visualization with multi-line tooltips...\")\n",
        "fig = px.scatter(\n",
        "    data_frame=rt_df,\n",
        "    x='pca_1',\n",
        "    y='pca_2',\n",
        "    color='log_conductivity',\n",
        "    color_continuous_scale=px.colors.sequential.Viridis,\n",
        "\n",
        "    # Pass the columns we want to show in the tooltip to custom_data\n",
        "    # The order here is important and matches the indices in the hovertemplate below.\n",
        "    custom_data=[\n",
        "        'feature_text_wrapped',        # customdata[0]\n",
        "        'acronym',                     # customdata[1]\n",
        "        'ionic_conductivity_S_per_cm', # customdata[2]\n",
        "        'paper_title'                  # customdata[3]\n",
        "    ],\n",
        "\n",
        "    labels={\n",
        "        \"pca_1\": \"Principal Component 1\",\n",
        "        \"pca_2\": \"Principal Component 2\",\n",
        "        \"log_conductivity\": \"Log10(Conductivity)\"\n",
        "    },\n",
        "    title='Interactive PCA of Room Temperature Materials'\n",
        ")\n",
        "\n",
        "# --- Define the Custom Hover Template ---\n",
        "# This template uses HTML and references the columns from custom_data by index.\n",
        "fig.update_traces(\n",
        "    hovertemplate=(\n",
        "        \"<b>%{customdata[1]}</b><br><br>\" +  # Show Acronym in bold\n",
        "        \"<b>Description:</b><br>%{customdata[0]}<br><br>\" +  # Show the wrapped feature text\n",
        "        \"<b>Conductivity:</b> %{customdata[2]:.2e} S/cm<br>\" + # Show conductivity\n",
        "        \"<b>Paper:</b> %{customdata[3]}\" + # Show paper title\n",
        "        \"<extra></extra>\"  # This special tag hides the default trace name\n",
        "    )\n",
        ")\n",
        "\n",
        "# Improve the layout\n",
        "fig.update_layout(\n",
        "    width=900,\n",
        "    height=700,\n",
        "    title_font_size=20\n",
        ")\n",
        "\n",
        "# Show the interactive plot directly in the notebook\n",
        "fig.show()\n",
        "\n",
        "# Optionally, save the plot to a self-contained HTML file\n",
        "html_output_filename = \"interactive_conductivity_pca_wrapped.html\"\n",
        "fig.write_html(html_output_filename)\n",
        "print(f\"Interactive plot also saved to '{html_output_filename}'\")"
      ],
      "metadata": {
        "id": "mw-TFyQUb7w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From that alloy example-Leon"
      ],
      "metadata": {
        "id": "S7PpI324DDzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_enhanced_feature_texts(df, strategy='comprehensive'):\n",
        "    \"\"\"\n",
        "    Create feature text with different strategies for embedding generation.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with material data (your rt_df)\n",
        "        strategy: Strategy for creating text\n",
        "            - 'comprehensive': All available information\n",
        "            - 'material_only': Focus on material composition\n",
        "            - 'processing_focused': Emphasize processing methods\n",
        "            - 'description_focused': Prioritize material description\n",
        "\n",
        "    Returns:\n",
        "        Series with feature text for each material\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Creating feature text using '{strategy}' strategy\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    if strategy == 'comprehensive':\n",
        "        # Include all available information\n",
        "        feature_text = (\n",
        "            \"Material: \" + df['full_name'].fillna('Unknown') +\n",
        "            \" (\" + df['acronym'].fillna('') + \"). \" +\n",
        "            \"Class: \" + df['material_class'].fillna('Unknown') + \". \" +\n",
        "            \"Description: \" + df['material_description'].fillna('No description') + \". \" +\n",
        "            \"Processing: \" + df['processing_method'].fillna('Not specified') + \". \" +\n",
        "            \"Source: \" + df['source_location'].fillna('Unknown')\n",
        "        )\n",
        "\n",
        "    elif strategy == 'material_only':\n",
        "        # Focus on material composition\n",
        "        feature_text = (\n",
        "            \"Material: \" + df['full_name'].fillna('Unknown') +\n",
        "            \" (\" + df['acronym'].fillna('') + \"). \" +\n",
        "            \"Material class: \" + df['material_class'].fillna('Unknown')\n",
        "        )\n",
        "\n",
        "    elif strategy == 'processing_focused':\n",
        "        # Emphasize processing methods\n",
        "        feature_text = (\n",
        "            \"Material: \" + df['full_name'].fillna('Unknown') + \". \" +\n",
        "            \"Processing method: \" + df['processing_method'].fillna('Not specified') + \". \" +\n",
        "            \"Material class: \" + df['material_class'].fillna('Unknown')\n",
        "        )\n",
        "\n",
        "    elif strategy == 'description_focused':\n",
        "        # Prioritize material description\n",
        "        feature_text = (\n",
        "            \"Material: \" + df['full_name'].fillna('Unknown') + \". \" +\n",
        "            \"Description: \" + df['material_description'].fillna('No description') + \". \" +\n",
        "            \"Class: \" + df['material_class'].fillna('Unknown')\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy: {strategy}. Choose from: 'comprehensive', 'material_only', 'processing_focused', 'description_focused'\")\n",
        "\n",
        "    # Print examples\n",
        "    print(f\"\\n📊 Created {len(feature_text)} feature texts\")\n",
        "    print(f\"\\n📝 Example of first 3 materials:\\n\")\n",
        "    for i in range(min(3, len(feature_text))):\n",
        "        print(f\"Material {i+1}:\")\n",
        "        print(f\"  {feature_text.iloc[i][:200]}...\")  # First 200 chars\n",
        "        print()\n",
        "\n",
        "    # Print statistics\n",
        "    text_lengths = feature_text.str.len()\n",
        "    print(f\"📏 Text length statistics:\")\n",
        "    print(f\"  Mean length: {text_lengths.mean():.0f} characters\")\n",
        "    print(f\"  Min length:  {text_lengths.min():.0f} characters\")\n",
        "    print(f\"  Max length:  {text_lengths.max():.0f} characters\")\n",
        "\n",
        "    return feature_text\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TEST THE FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"=\"*60)\n",
        "    print(\"TESTING FEATURE TEXT CREATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Assuming you have your rt_df loaded, test each strategy\n",
        "    print(\"\\n🧪 Test 1: Comprehensive strategy\")\n",
        "    text_comprehensive = create_enhanced_feature_texts(rt_df, strategy='comprehensive')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\\n🧪 Test 2: Material-only strategy\")\n",
        "    text_material = create_enhanced_feature_texts(rt_df, strategy='material_only')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\\n🧪 Test 3: Processing-focused strategy\")\n",
        "    text_processing = create_enhanced_feature_texts(rt_df, strategy='processing_focused')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\\n🧪 Test 4: Description-focused strategy\")\n",
        "    text_description = create_enhanced_feature_texts(rt_df, strategy='description_focused')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅ All tests completed!\")\n",
        "    print(f\"\\nYou can now use these feature texts for embedding generation.\")\n",
        "    print(f\"Example: rt_df['feature_text'] = create_enhanced_feature_texts(rt_df, 'comprehensive')\")"
      ],
      "metadata": {
        "id": "RyD_FrFyDGur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it with your rt_df\n",
        "text_comprehensive = create_enhanced_feature_texts(rt_df, strategy='comprehensive')\n",
        "\n",
        "# Compare with your current approach\n",
        "print(\"\\nYour current feature text:\")\n",
        "print(rt_df['feature_text'].iloc[0])\n",
        "\n",
        "print(\"\\nNew comprehensive text:\")\n",
        "print(text_comprehensive.iloc[0])"
      ],
      "metadata": {
        "id": "hbptHmzPDH19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_embeddings_with_pca(df, embedding_provider, embedding_model,\n",
        "                                 feature_text_column='feature_text',\n",
        "                                 n_pca_components=50):\n",
        "    \"\"\"\n",
        "    Generate embeddings from text and apply PCA reduction with proper standardization.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with material data\n",
        "        embedding_provider: Provider name (e.g., 'openai', 'google')\n",
        "        embedding_model: Model name\n",
        "        feature_text_column: Column name containing the text to embed\n",
        "        n_pca_components: Number of PCA components to keep (default: 50)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (df_with_pca, pca_model, scaler_model, original_embeddings)\n",
        "            - df_with_pca: Original df with added Embedding_PC1, Embedding_PC2, etc. columns\n",
        "            - pca_model: Fitted PCA model (for later analysis)\n",
        "            - scaler_model: Fitted StandardScaler (for consistency)\n",
        "            - original_embeddings: Raw embedding array before PCA\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"GENERATING EMBEDDINGS WITH PCA REDUCTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Generate embeddings\n",
        "    print(f\"\\n🔄 Generating embeddings using {embedding_provider}/{embedding_model}...\")\n",
        "    embeddings = get_embeddings(\n",
        "        provider=embedding_provider,\n",
        "        model_name=embedding_model,\n",
        "        texts=df[feature_text_column].tolist()\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Generated embeddings shape: {embeddings.shape}\")\n",
        "    print(f\"   Samples: {embeddings.shape[0]}\")\n",
        "    print(f\"   Original dimensions: {embeddings.shape[1]}\")\n",
        "\n",
        "    # Step 2: Filter out failed embeddings (all zeros)\n",
        "    valid_mask = np.all(embeddings != 0, axis=1)\n",
        "    n_failed = (~valid_mask).sum()\n",
        "\n",
        "    if n_failed > 0:\n",
        "        print(f\"⚠️  Warning: {n_failed} embeddings failed (all zeros). Removing them.\")\n",
        "\n",
        "    df_filtered = df[valid_mask].copy()\n",
        "    embeddings_filtered = embeddings[valid_mask]\n",
        "\n",
        "    print(f\"✅ Valid embeddings: {len(df_filtered)}\")\n",
        "\n",
        "    # Step 3: Standardize embeddings (IMPORTANT for PCA!)\n",
        "    print(f\"\\n📊 Standardizing embeddings (zero mean, unit variance)...\")\n",
        "    scaler = StandardScaler()\n",
        "    embeddings_scaled = scaler.fit_transform(embeddings_filtered)\n",
        "\n",
        "    print(f\"   Mean after scaling: {embeddings_scaled.mean():.6f}\")\n",
        "    print(f\"   Std after scaling:  {embeddings_scaled.std():.6f}\")\n",
        "\n",
        "    # Step 4: Apply PCA\n",
        "    if n_pca_components > embeddings_filtered.shape[1]:\n",
        "        n_pca_components = embeddings_filtered.shape[1]\n",
        "        print(f\"⚠️  Reduced n_components to {n_pca_components} (max available)\")\n",
        "\n",
        "    print(f\"\\n🔬 Applying PCA reduction: {embeddings_filtered.shape[1]} → {n_pca_components} dimensions...\")\n",
        "    pca = PCA(n_components=n_pca_components, random_state=42)\n",
        "    embeddings_pca = pca.fit_transform(embeddings_scaled)\n",
        "\n",
        "    # Step 5: Create DataFrame with PCA components\n",
        "    pca_columns = [f'Embedding_PC{i+1}' for i in range(n_pca_components)]\n",
        "    pca_df = pd.DataFrame(embeddings_pca, columns=pca_columns, index=df_filtered.index)\n",
        "\n",
        "    # Combine with original dataframe\n",
        "    df_with_pca = pd.concat([df_filtered, pca_df], axis=1)\n",
        "\n",
        "    # Step 6: Print PCA statistics\n",
        "    print(f\"\\n✅ PCA completed!\")\n",
        "    print(f\"   Final shape: {df_with_pca.shape}\")\n",
        "    print(f\"   PCA components: {n_pca_components}\")\n",
        "\n",
        "    print(f\"\\n📈 Variance explained by first 10 components:\")\n",
        "    for i in range(min(10, n_pca_components)):\n",
        "        print(f\"   PC{i+1}: {pca.explained_variance_ratio_[i]:.4f} ({pca.explained_variance_ratio_[i]*100:.2f}%)\")\n",
        "\n",
        "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "    print(f\"\\n📊 Cumulative variance explained:\")\n",
        "    for threshold in [0.5, 0.7, 0.8, 0.9, 0.95]:\n",
        "        n_components_needed = np.argmax(cumulative_variance >= threshold) + 1\n",
        "        print(f\"   {threshold*100:.0f}% variance: {n_components_needed} components\")\n",
        "\n",
        "    print(f\"\\n   Total variance explained by all {n_pca_components} components: {cumulative_variance[-1]:.4f}\")\n",
        "\n",
        "    # Step 7: Create variance explained plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot 1: Scree plot (individual variance)\n",
        "    ax1.bar(range(1, min(21, n_pca_components+1)),\n",
        "            pca.explained_variance_ratio_[:20],\n",
        "            alpha=0.7, color='steelblue', edgecolor='black')\n",
        "    ax1.set_xlabel('Principal Component', fontsize=12)\n",
        "    ax1.set_ylabel('Variance Explained Ratio', fontsize=12)\n",
        "    ax1.set_title('Scree Plot: Variance per Component (First 20)', fontsize=13, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Cumulative variance\n",
        "    ax2.plot(range(1, n_pca_components+1), cumulative_variance, 'o-', linewidth=2, markersize=4)\n",
        "    ax2.axhline(y=0.9, color='r', linestyle='--', label='90% variance')\n",
        "    ax2.axhline(y=0.95, color='orange', linestyle='--', label='95% variance')\n",
        "    ax2.set_xlabel('Number of Components', fontsize=12)\n",
        "    ax2.set_ylabel('Cumulative Variance Explained', fontsize=12)\n",
        "    ax2.set_title('Cumulative Variance Explained', fontsize=13, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n💡 TIP: You can now use 'Embedding_PC1', 'Embedding_PC2', etc. as features in your models\")\n",
        "    print(f\"         or for visualization (e.g., scatter plot with PC1 vs PC2)\")\n",
        "\n",
        "    return df_with_pca, pca, scaler, embeddings_filtered\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TEST THE FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"=\"*80)\n",
        "    print(\"TESTING EMBEDDING + PCA PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # First, create feature text (using the function from before)\n",
        "    print(\"\\n📝 Step 1: Creating feature text...\")\n",
        "    rt_df['feature_text_new'] = create_enhanced_feature_texts(rt_df, strategy='comprehensive')\n",
        "\n",
        "    # Generate embeddings with PCA\n",
        "    print(\"\\n🚀 Step 2: Generating embeddings and applying PCA...\")\n",
        "    df_with_embeddings, pca_model, scaler_model, raw_embeddings = generate_embeddings_with_pca(\n",
        "        df=rt_df,\n",
        "        embedding_provider=EMBEDDING_PROVIDER,\n",
        "        embedding_model=EMBEDDING_MODEL,\n",
        "        feature_text_column='feature_text_new',\n",
        "        n_pca_components=50  # Start with 50, adjust based on variance plot\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"✅ EMBEDDING + PCA COMPLETED!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\n📋 New columns added:\")\n",
        "    embedding_cols = [col for col in df_with_embeddings.columns if col.startswith('Embedding_PC')]\n",
        "    print(f\"   {embedding_cols[:5]} ... (and {len(embedding_cols)-5} more)\")\n",
        "\n",
        "    print(f\"\\n🎯 You now have:\")\n",
        "    print(f\"   • df_with_embeddings: Your data with PCA components\")\n",
        "    print(f\"   • pca_model: The fitted PCA model\")\n",
        "    print(f\"   • scaler_model: The fitted StandardScaler\")\n",
        "    print(f\"   • raw_embeddings: Original embedding vectors (before PCA)\")"
      ],
      "metadata": {
        "id": "mWFmxYyBDbWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_pca_embeddings(df_with_embeddings, target_column='log_conductivity'):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualizations of PCA-reduced embeddings.\n",
        "\n",
        "    Args:\n",
        "        df_with_embeddings: DataFrame with Embedding_PC columns\n",
        "        target_column: Column to use for coloring (default: 'log_conductivity')\n",
        "\n",
        "    Returns:\n",
        "        matplotlib figure\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"CREATING PCA VISUALIZATIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "    # Plot 1: PC1 vs PC2 colored by conductivity\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    scatter1 = ax1.scatter(df_with_embeddings['Embedding_PC1'],\n",
        "                          df_with_embeddings['Embedding_PC2'],\n",
        "                          c=df_with_embeddings[target_column],\n",
        "                          cmap='viridis', s=100, alpha=0.7,\n",
        "                          edgecolors='black', linewidth=0.5)\n",
        "    cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
        "    cbar1.set_label('Log₁₀(Conductivity)', fontsize=10)\n",
        "    ax1.set_xlabel('Principal Component 1 (14.8% var)', fontsize=11)\n",
        "    ax1.set_ylabel('Principal Component 2 (6.7% var)', fontsize=11)\n",
        "    ax1.set_title('PCA: Colored by Conductivity', fontsize=12, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: PC1 vs PC2 colored by material class\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    material_classes = df_with_embeddings['material_class'].unique()\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(material_classes)))\n",
        "\n",
        "    for mat_class, color in zip(material_classes, colors):\n",
        "        mask = df_with_embeddings['material_class'] == mat_class\n",
        "        ax2.scatter(df_with_embeddings.loc[mask, 'Embedding_PC1'],\n",
        "                   df_with_embeddings.loc[mask, 'Embedding_PC2'],\n",
        "                   label=mat_class, color=color, s=100, alpha=0.7,\n",
        "                   edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    ax2.set_xlabel('Principal Component 1', fontsize=11)\n",
        "    ax2.set_ylabel('Principal Component 2', fontsize=11)\n",
        "    ax2.set_title('PCA: Colored by Material Class', fontsize=12, fontweight='bold')\n",
        "    ax2.legend(fontsize=9, loc='best')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: PC1 vs PC3 colored by conductivity\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    scatter3 = ax3.scatter(df_with_embeddings['Embedding_PC1'],\n",
        "                          df_with_embeddings['Embedding_PC3'],\n",
        "                          c=df_with_embeddings[target_column],\n",
        "                          cmap='plasma', s=100, alpha=0.7,\n",
        "                          edgecolors='black', linewidth=0.5)\n",
        "    cbar3 = plt.colorbar(scatter3, ax=ax3)\n",
        "    cbar3.set_label('Log₁₀(Conductivity)', fontsize=10)\n",
        "    ax3.set_xlabel('Principal Component 1 (14.8% var)', fontsize=11)\n",
        "    ax3.set_ylabel('Principal Component 3 (3.9% var)', fontsize=11)\n",
        "    ax3.set_title('PCA: PC1 vs PC3', fontsize=12, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Conductivity distribution by material class\n",
        "    ax4 = plt.subplot(2, 3, 4)\n",
        "    material_classes_sorted = df_with_embeddings.groupby('material_class')[target_column].median().sort_values().index\n",
        "\n",
        "    data_by_class = [df_with_embeddings[df_with_embeddings['material_class'] == mc][target_column].values\n",
        "                     for mc in material_classes_sorted]\n",
        "\n",
        "    bp = ax4.boxplot(data_by_class, labels=material_classes_sorted, patch_artist=True)\n",
        "    for patch, color in zip(bp['boxes'], plt.cm.Set3(np.linspace(0, 1, len(material_classes_sorted)))):\n",
        "        patch.set_facecolor(color)\n",
        "\n",
        "    ax4.set_ylabel('Log₁₀(Conductivity)', fontsize=11)\n",
        "    ax4.set_title('Conductivity by Material Class', fontsize=12, fontweight='bold')\n",
        "    ax4.tick_params(axis='x', rotation=45, labelsize=9)\n",
        "    ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Plot 5: PC1 distribution\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "    ax5.hist(df_with_embeddings['Embedding_PC1'], bins=20, alpha=0.7,\n",
        "            color='steelblue', edgecolor='black')\n",
        "    ax5.axvline(df_with_embeddings['Embedding_PC1'].mean(), color='red',\n",
        "               linestyle='--', linewidth=2, label='Mean')\n",
        "    ax5.set_xlabel('Principal Component 1', fontsize=11)\n",
        "    ax5.set_ylabel('Frequency', fontsize=11)\n",
        "    ax5.set_title('Distribution of PC1', fontsize=12, fontweight='bold')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 6: Correlation between PC1 and conductivity\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    ax6.scatter(df_with_embeddings['Embedding_PC1'],\n",
        "               df_with_embeddings[target_column],\n",
        "               alpha=0.6, s=80, color='coral', edgecolors='black', linewidth=0.5)\n",
        "\n",
        "    # Add trend line\n",
        "    z = np.polyfit(df_with_embeddings['Embedding_PC1'], df_with_embeddings[target_column], 1)\n",
        "    p = np.poly1d(z)\n",
        "    x_line = np.linspace(df_with_embeddings['Embedding_PC1'].min(),\n",
        "                        df_with_embeddings['Embedding_PC1'].max(), 100)\n",
        "    ax6.plot(x_line, p(x_line), \"r--\", linewidth=2, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
        "\n",
        "    # Calculate correlation\n",
        "    corr = df_with_embeddings['Embedding_PC1'].corr(df_with_embeddings[target_column])\n",
        "    ax6.set_xlabel('Principal Component 1', fontsize=11)\n",
        "    ax6.set_ylabel('Log₁₀(Conductivity)', fontsize=11)\n",
        "    ax6.set_title(f'PC1 vs Conductivity (r={corr:.3f})', fontsize=12, fontweight='bold')\n",
        "    ax6.legend(fontsize=9)\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('pca_embeddings_visualization.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✅ Saved visualization to 'pca_embeddings_visualization.png'\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print correlation analysis\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CORRELATION ANALYSIS: PCA COMPONENTS vs CONDUCTIVITY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    pc_cols = [col for col in df_with_embeddings.columns if col.startswith('Embedding_PC')][:10]\n",
        "    correlations = [(col, df_with_embeddings[col].corr(df_with_embeddings[target_column]))\n",
        "                   for col in pc_cols]\n",
        "    correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    print(\"\\nTop 10 PCs by correlation with conductivity:\")\n",
        "    for i, (pc, corr) in enumerate(correlations, 1):\n",
        "        print(f\"  {i:2d}. {pc:20s}: {corr:+.4f}\")\n",
        "\n",
        "    # Material class statistics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MATERIAL CLASS STATISTICS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    class_stats = df_with_embeddings.groupby('material_class').agg({\n",
        "        target_column: ['count', 'mean', 'std', 'min', 'max']\n",
        "    }).round(3)\n",
        "    print(class_stats)\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TEST THE VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"=\"*80)\n",
        "    print(\"VISUALIZING PCA EMBEDDINGS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create visualizations\n",
        "    fig = visualize_pca_embeddings(df_with_embeddings, target_column='log_conductivity')\n",
        "\n",
        "    print(\"\\n✅ Visualization complete!\")"
      ],
      "metadata": {
        "id": "peroGylfDzHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def quick_model_test(df_with_embeddings, n_pca_to_test=[5, 10, 20, 50],\n",
        "                     target_column='log_conductivity', n_iterations=10):\n",
        "    \"\"\"\n",
        "    Quick test: Can embeddings predict conductivity better than baseline?\n",
        "    Tests different numbers of PCA components.\n",
        "\n",
        "    Args:\n",
        "        df_with_embeddings: DataFrame with Embedding_PC columns\n",
        "        n_pca_to_test: List of PCA component counts to test\n",
        "        target_column: Target variable\n",
        "        n_iterations: Number of random train/test splits\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"QUICK MODEL TEST: DO EMBEDDINGS HELP PREDICT CONDUCTIVITY?\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Get available PCA columns\n",
        "    all_pca_cols = [col for col in df_with_embeddings.columns if col.startswith('Embedding_PC')]\n",
        "    max_available = len(all_pca_cols)\n",
        "\n",
        "    print(f\"\\n📊 Testing with {n_iterations} random train/test splits (80/20)\")\n",
        "    print(f\"   Available PCA components: {max_available}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Baseline: Predict mean (simplest possible model)\n",
        "    print(f\"\\n🎯 Baseline: Always predict the mean...\")\n",
        "    y = df_with_embeddings[target_column]\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2,\n",
        "                                               random_state=42+iteration)\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Predict mean\n",
        "        y_pred_mean = np.full(len(y_test), y_train.mean())\n",
        "\n",
        "        results.append({\n",
        "            'n_components': 0,\n",
        "            'model': 'Baseline (mean)',\n",
        "            'iteration': iteration + 1,\n",
        "            'r2': r2_score(y_test, y_pred_mean),\n",
        "            'mse': mean_squared_error(y_test, y_pred_mean)\n",
        "        })\n",
        "\n",
        "    print(f\"   Baseline R²: {np.mean([r['r2'] for r in results if r['model']=='Baseline (mean)']):.4f}\")\n",
        "\n",
        "    # Test with different numbers of PCA components\n",
        "    for n_pca in n_pca_to_test:\n",
        "        if n_pca > max_available:\n",
        "            print(f\"\\n⚠️  Skipping {n_pca} components (max available: {max_available})\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n🔬 Testing with {n_pca} PCA components...\")\n",
        "\n",
        "        # Select PCA columns\n",
        "        pca_cols = all_pca_cols[:n_pca]\n",
        "        X = df_with_embeddings[pca_cols]\n",
        "        y = df_with_embeddings[target_column]\n",
        "\n",
        "        for iteration in range(n_iterations):\n",
        "            # Split data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42+iteration\n",
        "            )\n",
        "\n",
        "            # Train Random Forest\n",
        "            rf = RandomForestRegressor(n_estimators=100, random_state=42+iteration, n_jobs=-1)\n",
        "            rf.fit(X_train, y_train)\n",
        "            y_pred = rf.predict(X_test)\n",
        "\n",
        "            results.append({\n",
        "                'n_components': n_pca,\n",
        "                'model': f'RF ({n_pca} PCs)',\n",
        "                'iteration': iteration + 1,\n",
        "                'r2': r2_score(y_test, y_pred),\n",
        "                'mse': mean_squared_error(y_test, y_pred)\n",
        "            })\n",
        "\n",
        "        avg_r2 = np.mean([r['r2'] for r in results if r['n_components']==n_pca])\n",
        "        avg_mse = np.mean([r['mse'] for r in results if r['n_components']==n_pca])\n",
        "        print(f\"   Avg R²: {avg_r2:.4f}, Avg MSE: {avg_mse:.4f}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUMMARY RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    summary = results_df.groupby(['model', 'n_components']).agg({\n",
        "        'r2': ['mean', 'std'],\n",
        "        'mse': ['mean', 'std']\n",
        "    }).round(4)\n",
        "\n",
        "    print(\"\\n📊 Performance by model:\")\n",
        "    print(summary)\n",
        "\n",
        "    # Create visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot 1: R² scores\n",
        "    models = results_df['model'].unique()\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(models)))\n",
        "\n",
        "    for model, color in zip(models, colors):\n",
        "        model_data = results_df[results_df['model'] == model]\n",
        "        ax1.scatter(model_data['iteration'], model_data['r2'],\n",
        "                   label=model, alpha=0.6, s=60, color=color)\n",
        "\n",
        "    ax1.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    ax1.set_xlabel('Iteration', fontsize=11)\n",
        "    ax1.set_ylabel('R² Score', fontsize=11)\n",
        "    ax1.set_title('R² Performance Across Iterations', fontsize=12, fontweight='bold')\n",
        "    ax1.legend(fontsize=9)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Boxplot comparison\n",
        "    box_data = [results_df[results_df['model'] == model]['r2'].values for model in models]\n",
        "    bp = ax2.boxplot(box_data, labels=[m.replace('RF (', '').replace(' PCs)', '') for m in models],\n",
        "                     patch_artist=True)\n",
        "\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.6)\n",
        "\n",
        "    ax2.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    ax2.set_ylabel('R² Score', fontsize=11)\n",
        "    ax2.set_title('R² Distribution by Model', fontsize=12, fontweight='bold')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('embedding_model_test.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Interpretation\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"💡 INTERPRETATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    baseline_r2 = results_df[results_df['model']=='Baseline (mean)']['r2'].mean()\n",
        "    best_model = summary['r2']['mean'].idxmax()[0]\n",
        "    best_r2 = summary.loc[(best_model, summary.loc[best_model].index[0]), ('r2', 'mean')]\n",
        "\n",
        "    print(f\"\\n• Baseline (predicting mean): R² = {baseline_r2:.4f}\")\n",
        "    print(f\"• Best embedding model: {best_model} with R² = {best_r2:.4f}\")\n",
        "\n",
        "    if best_r2 > 0.3:\n",
        "        print(f\"\\n✅ GOOD: Embeddings can predict conductivity (R² > 0.3)\")\n",
        "        print(f\"   The text descriptions contain useful information!\")\n",
        "    elif best_r2 > 0.1:\n",
        "        print(f\"\\n⚠️  WEAK: Embeddings show weak predictive power (0.1 < R² < 0.3)\")\n",
        "        print(f\"   Text has some signal, but may need additional features\")\n",
        "    else:\n",
        "        print(f\"\\n❌ POOR: Embeddings don't predict well (R² < 0.1)\")\n",
        "        print(f\"   The text descriptions may not capture conductivity drivers\")\n",
        "\n",
        "    print(f\"\\n🔍 Next steps:\")\n",
        "    if best_r2 < 0.3:\n",
        "        print(f\"   1. Add traditional features (temperature, processing indicators)\")\n",
        "        print(f\"   2. Try different embedding strategies (material_only, processing_focused)\")\n",
        "        print(f\"   3. Feature engineering from text (keyword extraction)\")\n",
        "    else:\n",
        "        print(f\"   1. Combine embeddings with traditional features\")\n",
        "        print(f\"   2. Feature selection to find most important components\")\n",
        "        print(f\"   3. Try different models (XGBoost, Neural Networks)\")\n",
        "\n",
        "    return results_df, fig\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RUN THE TEST\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Test if embeddings can predict conductivity\n",
        "    results_df, fig = quick_model_test(\n",
        "        df_with_embeddings,\n",
        "        n_pca_to_test=[5, 10, 20, 50],\n",
        "        target_column='log_conductivity',\n",
        "        n_iterations=10\n",
        "    )"
      ],
      "metadata": {
        "id": "fRs-iZxiEVe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_traditional_features(df):\n",
        "    \"\"\"\n",
        "    Extract traditional features from your data that might predict conductivity.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with material data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with traditional features only\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"CREATING TRADITIONAL FEATURES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    features = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # ========================================================================\n",
        "    # 1. TEMPERATURE (already numeric)\n",
        "    # ========================================================================\n",
        "    features['temperature_celsius'] = df['temperature_celsius']\n",
        "    print(f\"\\n✅ Temperature feature added\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 2. MATERIAL CLASS (one-hot encoding)\n",
        "    # ========================================================================\n",
        "    class_dummies = pd.get_dummies(df['material_class'], prefix='class', drop_first=False)\n",
        "    features = pd.concat([features, class_dummies], axis=1)\n",
        "    print(f\"✅ Material class features added: {list(class_dummies.columns)}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 3. PROCESSING METHOD INDICATORS (keyword detection)\n",
        "    # ========================================================================\n",
        "    processing_keywords = {\n",
        "        'sintering': ['sinter', 'sintered', 'sintering'],\n",
        "        'casting': ['cast', 'casting'],\n",
        "        'pressing': ['press', 'pressed', 'pressing', 'compression'],\n",
        "        'annealing': ['anneal', 'annealed', 'annealing'],\n",
        "        'drying': ['dry', 'dried', 'drying'],\n",
        "        'heating': ['heat', 'heated', 'heating'],\n",
        "        'milling': ['mill', 'milled', 'milling', 'ball-mill'],\n",
        "        'mixing': ['mix', 'mixed', 'mixing', 'blend'],\n",
        "        'electrospinning': ['electrospin'],\n",
        "        'calcination': ['calcin'],\n",
        "        'grinding': ['grind', 'ground'],\n",
        "        'coating': ['coat', 'coated', 'coating']\n",
        "    }\n",
        "\n",
        "    processing_text = df['processing_method'].fillna('').str.lower()\n",
        "\n",
        "    for feature_name, keywords in processing_keywords.items():\n",
        "        features[f'process_{feature_name}'] = processing_text.apply(\n",
        "            lambda x: int(any(kw in x for kw in keywords))\n",
        "        )\n",
        "\n",
        "    print(f\"✅ Processing features added: {len(processing_keywords)} indicators\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 4. MATERIAL NAME FEATURES (complexity indicators)\n",
        "    # ========================================================================\n",
        "\n",
        "    # Length of material name (proxy for complexity)\n",
        "    features['material_name_length'] = df['full_name'].fillna('').str.len()\n",
        "\n",
        "    # Has acronym\n",
        "    features['has_acronym'] = (~df['acronym'].isna()).astype(int)\n",
        "\n",
        "    # Number of words in name (composite materials often have more words)\n",
        "    features['name_word_count'] = df['full_name'].fillna('').str.split().str.len()\n",
        "\n",
        "    print(f\"✅ Material name features added: 3 features\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 5. DESCRIPTION FEATURES (information richness)\n",
        "    # ========================================================================\n",
        "\n",
        "    # Description length\n",
        "    features['description_length'] = df['material_description'].fillna('').str.len()\n",
        "\n",
        "    # Has description\n",
        "    features['has_description'] = (df['material_description'].fillna('').str.len() > 0).astype(int)\n",
        "\n",
        "    print(f\"✅ Description features added: 2 features\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 6. CHEMICAL ELEMENT INDICATORS (from material names)\n",
        "    # ========================================================================\n",
        "\n",
        "    common_elements = {\n",
        "        'Li': ['Li', 'Lithium'],\n",
        "        'La': ['La', 'Lanthanum'],\n",
        "        'Zr': ['Zr', 'Zircon'],\n",
        "        'Ti': ['Ti', 'Titan'],\n",
        "        'Al': ['Al', 'Aluminum', 'Aluminium'],\n",
        "        'Zn': ['Zn', 'Zinc'],\n",
        "        'O': ['Oxide', 'Oxygen'],\n",
        "        'F': ['Fluor', 'PVDF'],\n",
        "        'Ga': ['Ga', 'Gallium'],\n",
        "        'Ta': ['Ta', 'Tantalum'],\n",
        "        'Nb': ['Nb', 'Niobium']\n",
        "    }\n",
        "\n",
        "    material_text = df['full_name'].fillna('') + ' ' + df['acronym'].fillna('')\n",
        "    material_text = material_text.str.lower()\n",
        "\n",
        "    for element, keywords in common_elements.items():\n",
        "        features[f'element_{element}'] = material_text.apply(\n",
        "            lambda x: int(any(kw.lower() in x for kw in keywords))\n",
        "        )\n",
        "\n",
        "    print(f\"✅ Chemical element features added: {len(common_elements)} indicators\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 7. SOURCE LOCATION (where data was extracted from)\n",
        "    # ========================================================================\n",
        "\n",
        "    # Some sources might be more reliable than others\n",
        "    source_dummies = pd.get_dummies(df['source_location'].fillna('Unknown'),\n",
        "                                   prefix='source', drop_first=False)\n",
        "\n",
        "    # Only keep sources that appear at least 3 times\n",
        "    source_counts = df['source_location'].value_counts()\n",
        "    frequent_sources = source_counts[source_counts >= 3].index\n",
        "    source_dummies = source_dummies[[col for col in source_dummies.columns\n",
        "                                    if any(src in col for src in frequent_sources)]]\n",
        "\n",
        "    features = pd.concat([features, source_dummies], axis=1)\n",
        "    print(f\"✅ Source location features added: {len(source_dummies.columns)} indicators\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SUMMARY\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FEATURE SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nTotal features created: {len(features.columns)}\")\n",
        "    print(f\"Samples: {len(features)}\")\n",
        "    print(f\"\\nFeature types:\")\n",
        "    print(f\"  • Temperature: 1\")\n",
        "    print(f\"  • Material class: {len(class_dummies.columns)}\")\n",
        "    print(f\"  • Processing methods: {len(processing_keywords)}\")\n",
        "    print(f\"  • Material name: 3\")\n",
        "    print(f\"  • Description: 2\")\n",
        "    print(f\"  • Chemical elements: {len(common_elements)}\")\n",
        "    print(f\"  • Source location: {len(source_dummies.columns)}\")\n",
        "\n",
        "    # Check for missing values\n",
        "    missing = features.isnull().sum()\n",
        "    if missing.any():\n",
        "        print(f\"\\n⚠️  Missing values found:\")\n",
        "        print(missing[missing > 0])\n",
        "        print(f\"\\n   Filling missing values with 0...\")\n",
        "        features = features.fillna(0)\n",
        "    else:\n",
        "        print(f\"\\n✅ No missing values\")\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TEST TRADITIONAL FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"=\"*80)\n",
        "    print(\"TESTING TRADITIONAL FEATURES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create traditional features\n",
        "    traditional_features = create_traditional_features(df_with_embeddings)\n",
        "\n",
        "    # Test them with the same function\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TESTING: CAN TRADITIONAL FEATURES PREDICT CONDUCTIVITY?\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Add traditional features to a copy of the dataframe\n",
        "    df_with_traditional = df_with_embeddings.copy()\n",
        "    df_with_traditional = pd.concat([df_with_traditional, traditional_features], axis=1)\n",
        "\n",
        "    # Now test with traditional features\n",
        "    # We'll manually do a quick test\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "    # Get traditional feature columns\n",
        "    trad_cols = list(traditional_features.columns)\n",
        "\n",
        "    print(f\"\\n🔬 Testing Random Forest with {len(trad_cols)} traditional features...\")\n",
        "    print(f\"   Features: {trad_cols[:5]} ... (and {len(trad_cols)-5} more)\")\n",
        "\n",
        "    X = traditional_features\n",
        "    y = df_with_embeddings['log_conductivity']\n",
        "\n",
        "    results = []\n",
        "    for iteration in range(10):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42+iteration\n",
        "        )\n",
        "\n",
        "        rf = RandomForestRegressor(n_estimators=100, random_state=42+iteration, n_jobs=-1)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred = rf.predict(X_test)\n",
        "\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        results.append({'r2': r2, 'mse': mse})\n",
        "\n",
        "    avg_r2 = np.mean([r['r2'] for r in results])\n",
        "    std_r2 = np.std([r['r2'] for r in results])\n",
        "    avg_mse = np.mean([r['mse'] for r in results])\n",
        "\n",
        "    print(f\"\\n📊 RESULTS:\")\n",
        "    print(f\"   R² = {avg_r2:.4f} ± {std_r2:.4f}\")\n",
        "    print(f\"   MSE = {avg_mse:.4f}\")\n",
        "\n",
        "    if avg_r2 > 0.3:\n",
        "        print(f\"\\n✅ GOOD! Traditional features can predict conductivity\")\n",
        "    elif avg_r2 > 0.1:\n",
        "        print(f\"\\n⚠️  WEAK: Some predictive power but limited\")\n",
        "    else:\n",
        "        print(f\"\\n❌ POOR: Traditional features don't help much either\")\n",
        "\n",
        "    # Feature importance\n",
        "    print(f\"\\n🎯 Training final model to check feature importance...\")\n",
        "    rf_final = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_final.fit(X, y)\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': trad_cols,\n",
        "        'importance': rf_final.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\n📊 Top 10 most important traditional features:\")\n",
        "    for i, row in feature_importance.head(10).iterrows():\n",
        "        print(f\"   {row['feature']:40s}: {row['importance']:.4f}\")"
      ],
      "metadata": {
        "id": "jngFoJVVErTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diagnose_dataset(df, target_column='log_conductivity'):\n",
        "    \"\"\"\n",
        "    Comprehensive diagnostic to understand why models aren't working.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with material data\n",
        "        target_column: Target variable column\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"DATASET DIAGNOSTIC ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # ========================================================================\n",
        "    # 1. SAMPLE SIZE AND VARIANCE\n",
        "    # ========================================================================\n",
        "    print(\"\\n📊 SAMPLE SIZE AND TARGET VARIANCE\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    y = df[target_column]\n",
        "    print(f\"Number of samples: {len(y)}\")\n",
        "    print(f\"Target mean: {y.mean():.4f}\")\n",
        "    print(f\"Target std: {y.std():.4f}\")\n",
        "    print(f\"Target range: [{y.min():.4f}, {y.max():.4f}]\")\n",
        "    print(f\"Coefficient of variation: {(y.std() / abs(y.mean())):.4f}\")\n",
        "\n",
        "    # Check for outliers\n",
        "    q1, q3 = y.quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    outliers = ((y < q1 - 1.5*iqr) | (y > q3 + 1.5*iqr)).sum()\n",
        "    print(f\"Potential outliers (IQR method): {outliers} ({outliers/len(y)*100:.1f}%)\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 2. TEMPERATURE ANALYSIS\n",
        "    # ========================================================================\n",
        "    print(\"\\n🌡️  TEMPERATURE ANALYSIS\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    temp = df['temperature_celsius']\n",
        "    print(f\"Temperature range: [{temp.min():.1f}, {temp.max():.1f}]°C\")\n",
        "    print(f\"Temperature std: {temp.std():.4f}\")\n",
        "    print(f\"Unique temperatures: {temp.nunique()}\")\n",
        "\n",
        "    if temp.std() < 5:\n",
        "        print(\"⚠️  WARNING: Temperature has very low variance!\")\n",
        "        print(\"   All data is essentially at the same temperature.\")\n",
        "        print(\"   This means temperature cannot predict conductivity.\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 3. MATERIAL CLASS DISTRIBUTION\n",
        "    # ========================================================================\n",
        "    print(\"\\n🧪 MATERIAL CLASS DISTRIBUTION\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    class_counts = df['material_class'].value_counts()\n",
        "    print(class_counts)\n",
        "\n",
        "    print(f\"\\n   Conductivity by class:\")\n",
        "    for mat_class in df['material_class'].unique():\n",
        "        class_data = df[df['material_class'] == mat_class][target_column]\n",
        "        print(f\"   {mat_class:12s}: mean={class_data.mean():7.3f}, std={class_data.std():6.3f}, n={len(class_data):3d}\")\n",
        "\n",
        "    # Check if classes overlap too much\n",
        "    print(\"\\n   Class separation (ANOVA F-statistic):\")\n",
        "    from scipy import stats\n",
        "    groups = [df[df['material_class'] == mc][target_column].values\n",
        "              for mc in df['material_class'].unique()]\n",
        "    f_stat, p_value = stats.f_oneway(*groups)\n",
        "    print(f\"   F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "    if p_value > 0.05:\n",
        "        print(\"   ⚠️  Classes do NOT have significantly different conductivities\")\n",
        "    else:\n",
        "        print(\"   ✅ Classes have significantly different conductivities\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 4. CHECK DATA QUALITY ISSUES\n",
        "    # ========================================================================\n",
        "    print(\"\\n🔍 DATA QUALITY CHECKS\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Check for duplicate materials\n",
        "    duplicates = df.duplicated(subset=['full_name'], keep=False).sum()\n",
        "    print(f\"Duplicate material names: {duplicates}\")\n",
        "\n",
        "    # Check text quality\n",
        "    empty_descriptions = (df['material_description'].fillna('').str.len() == 0).sum()\n",
        "    empty_processing = (df['processing_method'].fillna('').str.len() == 0).sum()\n",
        "\n",
        "    print(f\"Empty descriptions: {empty_descriptions} ({empty_descriptions/len(df)*100:.1f}%)\")\n",
        "    print(f\"Empty processing methods: {empty_processing} ({empty_processing/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # 5. VISUALIZATION\n",
        "    # ========================================================================\n",
        "    print(\"\\n📈 Creating diagnostic plots...\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Plot 1: Target distribution\n",
        "    ax = axes[0, 0]\n",
        "    ax.hist(y, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "    ax.axvline(y.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "    ax.axvline(y.median(), color='orange', linestyle='--', linewidth=2, label='Median')\n",
        "    ax.set_xlabel('Log₁₀(Conductivity)', fontsize=11)\n",
        "    ax.set_ylabel('Frequency', fontsize=11)\n",
        "    ax.set_title('Target Variable Distribution', fontsize=12, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Q-Q plot (check normality)\n",
        "    ax = axes[0, 1]\n",
        "    stats.probplot(y, dist=\"norm\", plot=ax)\n",
        "    ax.set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Conductivity by material class\n",
        "    ax = axes[0, 2]\n",
        "    class_data = [df[df['material_class'] == mc][target_column].values\n",
        "                  for mc in df['material_class'].unique()]\n",
        "    bp = ax.boxplot(class_data, labels=df['material_class'].unique(), patch_artist=True)\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "    ax.set_ylabel('Log₁₀(Conductivity)', fontsize=11)\n",
        "    ax.set_title('Conductivity by Material Class', fontsize=12, fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Sample size per class\n",
        "    ax = axes[1, 0]\n",
        "    class_counts.plot(kind='bar', ax=ax, color='coral', alpha=0.7, edgecolor='black')\n",
        "    ax.set_ylabel('Number of Samples', fontsize=11)\n",
        "    ax.set_title('Sample Size per Class', fontsize=12, fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 5: Conductivity vs temperature\n",
        "    ax = axes[1, 1]\n",
        "    ax.scatter(df['temperature_celsius'], y, alpha=0.6, s=60, color='green', edgecolors='black')\n",
        "    ax.set_xlabel('Temperature (°C)', fontsize=11)\n",
        "    ax.set_ylabel('Log₁₀(Conductivity)', fontsize=11)\n",
        "    ax.set_title('Conductivity vs Temperature', fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 6: Variance decomposition\n",
        "    ax = axes[1, 2]\n",
        "\n",
        "    # Calculate variance components\n",
        "    total_var = y.var()\n",
        "    within_class_var = sum(\n",
        "        len(df[df['material_class'] == mc]) * df[df['material_class'] == mc][target_column].var()\n",
        "        for mc in df['material_class'].unique()\n",
        "    ) / len(df)\n",
        "    between_class_var = total_var - within_class_var\n",
        "\n",
        "    ax.bar(['Within\\nClass', 'Between\\nClass', 'Total'],\n",
        "           [within_class_var, between_class_var, total_var],\n",
        "           color=['lightcoral', 'lightgreen', 'lightblue'],\n",
        "           alpha=0.7, edgecolor='black')\n",
        "    ax.set_ylabel('Variance', fontsize=11)\n",
        "    ax.set_title('Variance Decomposition', fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add percentage labels\n",
        "    ax.text(0, within_class_var/2, f'{within_class_var/total_var*100:.1f}%',\n",
        "            ha='center', fontsize=10, fontweight='bold')\n",
        "    ax.text(1, between_class_var/2, f'{between_class_var/total_var*100:.1f}%',\n",
        "            ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dataset_diagnostic.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # ========================================================================\n",
        "    # 6. RECOMMENDATIONS\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"💡 RECOMMENDATIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    issues = []\n",
        "    recommendations = []\n",
        "\n",
        "    if len(y) < 200:\n",
        "        issues.append(\"Small sample size (<200)\")\n",
        "        recommendations.append(\"Collect more data if possible\")\n",
        "\n",
        "    if temp.std() < 5:\n",
        "        issues.append(\"Temperature has no variance (all room temp)\")\n",
        "        recommendations.append(\"Filter out temperature as a feature OR collect data at different temps\")\n",
        "\n",
        "    if p_value > 0.05:\n",
        "        issues.append(\"Material classes don't separate conductivity well\")\n",
        "        recommendations.append(\"Look for other grouping variables (e.g., processing methods)\")\n",
        "\n",
        "    if outliers > len(y) * 0.1:\n",
        "        issues.append(f\"High number of outliers ({outliers/len(y)*100:.1f}%)\")\n",
        "        recommendations.append(\"Investigate and possibly remove extreme outliers\")\n",
        "\n",
        "    if within_class_var / total_var > 0.8:\n",
        "        issues.append(\"Most variance is WITHIN classes, not between them\")\n",
        "        recommendations.append(\"Material class alone won't predict well - need other features\")\n",
        "\n",
        "    if issues:\n",
        "        print(\"\\n⚠️  Issues detected:\")\n",
        "        for i, issue in enumerate(issues, 1):\n",
        "            print(f\"   {i}. {issue}\")\n",
        "\n",
        "        print(\"\\n🔧 Recommendations:\")\n",
        "        for i, rec in enumerate(recommendations, 1):\n",
        "            print(f\"   {i}. {rec}\")\n",
        "    else:\n",
        "        print(\"\\n✅ No major issues detected with dataset structure\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"🎯 BOTTOM LINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if len(y) < 200 and within_class_var / total_var > 0.8:\n",
        "        print(\"\\nYour dataset has:\")\n",
        "        print(\"  • Small sample size\")\n",
        "        print(\"  • High within-class variance\")\n",
        "        print(\"  • Limited distinguishing features\")\n",
        "        print(\"\\nThis makes prediction very difficult!\")\n",
        "        print(\"\\nBest approach: Focus on DESCRIPTIVE analysis rather than prediction\")\n",
        "        print(\"  - What material classes perform best on average?\")\n",
        "        print(\"  - What processing methods correlate with high conductivity?\")\n",
        "        print(\"  - Can you identify patterns manually in the data?\")\n",
        "    else:\n",
        "        print(\"\\nYou have enough signal for prediction. Try:\")\n",
        "        print(\"  - Feature engineering (extract more from text)\")\n",
        "        print(\"  - Different embedding strategies\")\n",
        "        print(\"  - Combined traditional + embedding features\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RUN DIAGNOSTIC\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    diagnose_dataset(df_with_embeddings, target_column='log_conductivity')"
      ],
      "metadata": {
        "id": "_J619uY3FCMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3RpwH-opDcTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train ML model"
      ],
      "metadata": {
        "id": "dw3Nax57axDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "T5Fm_UxKay_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Cell 1, add these new imports\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, BayesianRidge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "xmHRUCRMa03S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the features (X) and the target (y) from your cleaned, room-temperature DataFrame\n",
        "X = embeddings\n",
        "y = rt_df['log_conductivity'].values\n",
        "\n",
        "# Create a dictionary of all the models we want to test\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Lasso\": Lasso(alpha=0.1, random_state=42),\n",
        "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
        "    \"Bayesian Ridge\": BayesianRidge(),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"SVR\": SVR(kernel='rbf'),\n",
        "    \"PLS Regression\": PLSRegression(n_components=10),\n",
        "    \"XGBoost\": xgb.XGBRegressor(objective='reg:squarederror', n_estimators=500, random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "zTgX1AGBWLNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Running 5-fold cross-validation for each model...\")\n",
        "for name, model in tqdm(models.items(), desc=\"Validating Models\"):\n",
        "    # Calculate R-squared scores\n",
        "    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
        "\n",
        "    # Calculate Mean Absolute Error scores\n",
        "    mae_scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error')\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Mean R2\": np.mean(r2_scores),\n",
        "        \"Std R2\": np.std(r2_scores),\n",
        "        \"Mean MAE\": np.mean(mae_scores),\n",
        "        \"Std MAE\": np.std(mae_scores)\n",
        "    })\n",
        "\n",
        "# Display the results in a clean DataFrame\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"Mean R2\", ascending=False).reset_index(drop=True)\n",
        "print(\"\\n--- Cross-Validation Performance Summary ---\")\n",
        "display(results_df)"
      ],
      "metadata": {
        "id": "xxs74Wn0a-I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Split the data one time for consistent visualization\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the 2x4 subplot grid\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10)) # MODIFIED: Changed layout and size\n",
        "axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "print(\"\\n--- Generating Actual vs. Predicted Plots for Test Set ---\")\n",
        "for i, (name, model) in enumerate(models.items()):\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate test score\n",
        "    test_r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Create scatter plot\n",
        "    sns.scatterplot(x=y_test, y=y_pred, ax=ax, alpha=0.7, s=50, label=f'Test Data (R²={test_r2:.2f})')\n",
        "\n",
        "    # Plot the ideal fit line (y=x)\n",
        "    lims = [min(y_test.min(), y_pred.min()) - 1, max(y_test.max(), y_pred.max()) + 1]\n",
        "    ax.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label='Ideal Fit')\n",
        "    ax.set_xlim(lims)\n",
        "    ax.set_ylim(lims)\n",
        "\n",
        "    ax.set_title(f\"{name} Performance\", fontsize=12)\n",
        "    ax.set_xlabel(\"Actual Log10(Conductivity)\")\n",
        "    ax.set_ylabel(\"Predicted Log10(Conductivity)\")\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "    ax.legend(loc='upper left')\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "85IkNYlGa_t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Predict on New, Hypothetical Materials (Corrected)\n",
        "\n",
        "# --- 1. Automatically Select the Best Model ---\n",
        "# Find the model with the highest Mean R-squared from our cross-validation results\n",
        "best_model_name = results_df.loc[results_df['Mean R2'].idxmax()]['Model']\n",
        "best_model = models[best_model_name]\n",
        "print(f\"Selected the best model based on cross-validation: '{best_model_name}'\")\n",
        "\n",
        "# --- 2. Retrain the Best Model on the Entire Dataset ---\n",
        "# For final predictions, it's best practice to train the model on all available data\n",
        "print(f\"Retraining '{best_model_name}' on the full dataset...\")\n",
        "best_model.fit(X, y)\n",
        "print(\"Model retraining complete.\")\n",
        "\n",
        "# --- 3. Define Hypothetical Materials and Get Embeddings ---\n",
        "hypothetical_material_1 = \"Material: Ga-doped LLZO. Description: Dense ceramic pellet with cubic garnet phase. Processing: High-temperature sintering at 1200°C for 5 hours.\"\n",
        "hypothetical_material_2 = \"Material: PEO with LiTFSI. Description: Amorphous polymer film. Processing: Solution casting followed by drying in vacuum at 60°C.\"\n",
        "hypothetical_materials = [hypothetical_material_1, hypothetical_material_2]\n",
        "\n",
        "# Use the generic dispatcher function to get embeddings\n",
        "new_embeddings = get_embeddings(\n",
        "    provider=EMBEDDING_PROVIDER,\n",
        "    model_name=EMBEDDING_MODEL,\n",
        "    texts=hypothetical_materials\n",
        ")\n",
        "\n",
        "# --- 4. Make Predictions with the Best Model ---\n",
        "predicted_log_conductivities = best_model.predict(new_embeddings)\n",
        "\n",
        "# Convert log conductivity back to standard scientific notation for easier interpretation\n",
        "predicted_conductivities = [10**log_val for log_val in predicted_log_conductivities]\n",
        "\n",
        "# --- 5. Print the Results ---\n",
        "print(\"\\n--- Predictions for Hypothetical Materials ---\")\n",
        "print(f\"Using model: {best_model_name}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Material 1 (LLZO):\")\n",
        "print(f\"  - Predicted Log Conductivity: {predicted_log_conductivities[0]:.2f}\")\n",
        "print(f\"  - Predicted Conductivity (S/cm): {predicted_conductivities[0]:.2e}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Material 2 (PEO):\")\n",
        "print(f\"  - Predicted Log Conductivity: {predicted_log_conductivities[1]:.2f}\")\n",
        "print(f\"  - Predicted Conductivity (S/cm): {predicted_conductivities[1]:.2e}\")"
      ],
      "metadata": {
        "id": "TUh51H3rbHy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Re-evaluating Models with Scaling and PCA\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Create a new set of models wrapped in a preprocessing pipeline ---\n",
        "models_with_pca = {}\n",
        "print(\"Creating new models with StandardScaler and PCA preprocessing...\")\n",
        "for name, model in models.items():\n",
        "    # Each pipeline will first scale the data, then apply PCA, then train the model\n",
        "    models_with_pca[name] = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('pca', PCA(n_components=0.95, random_state=42)),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "# --- 2. Run 5-fold cross-validation on the new pipelines ---\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "pca_results = []\n",
        "\n",
        "print(\"\\nRunning 5-fold cross-validation on models with PCA...\")\n",
        "for name, pipeline in tqdm(models_with_pca.items(), desc=\"Validating PCA Models\"):\n",
        "    r2_scores = cross_val_score(pipeline, X, y, cv=kf, scoring='r2')\n",
        "    mae_scores = -cross_val_score(pipeline, X, y, cv=kf, scoring='neg_mean_absolute_error')\n",
        "\n",
        "    pca_results.append({\n",
        "        \"Model\": name,\n",
        "        \"Mean R2\": np.mean(r2_scores),\n",
        "        \"Mean MAE\": np.mean(mae_scores)\n",
        "    })\n",
        "\n",
        "pca_results_df = pd.DataFrame(pca_results)\n",
        "\n",
        "# --- 3. Create a Comparison DataFrame ---\n",
        "# Rename columns for clarity before merging\n",
        "original_renamed = results_df.rename(columns={'Mean R2': 'R2 (Original)', 'Mean MAE': 'MAE (Original)'})\n",
        "pca_renamed = pca_results_df.rename(columns={'Mean R2': 'R2 (With PCA)', 'Mean MAE': 'MAE (With PCA)'})\n",
        "\n",
        "# Merge the original and new results on the 'Model' column\n",
        "comparison_df = pd.merge(\n",
        "    original_renamed[['Model', 'R2 (Original)', 'MAE (Original)']],\n",
        "    pca_renamed[['Model', 'R2 (With PCA)', 'MAE (With PCA)']],\n",
        "    on='Model'\n",
        ")\n",
        "\n",
        "# Add a column to explicitly show the change in performance\n",
        "comparison_df['R2 Change'] = comparison_df['R2 (With PCA)'] - comparison_df['R2 (Original)']\n",
        "\n",
        "# Sort by the new R2 score to see the best-performing models at the top\n",
        "comparison_df = comparison_df.sort_values(by='R2 (With PCA)', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# --- 4. Display the Final Comparison ---\n",
        "print(\"\\n--- Performance Comparison: Original vs. Scaled + PCA ---\")\n",
        "display(comparison_df)"
      ],
      "metadata": {
        "id": "QiMZTInfdnc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}